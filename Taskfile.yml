# https://taskfile.dev

version: '3'

set: [errexit, pipefail]

tasks:
  default:
    cmds:
      - task: datendienst
      - task: lobid-download
      - task: lobid-filter
      - task: lobid-transform
      - task: wikidata-extract
      - task: wikidata-transform
      - task: commons-extract
      - task: commons-transform
      - task: output
    
  datendienst:
    desc: Download von GND-IDs aus dem DLA Datendienst
    dir: data
    cmds:
      - curl --silent --compressed 'https://dataservice.dla-marbach.de/v1/records?q=gnd_id_mv:%2A%20AND%20source:AK&fields=gnd_id_mv&format=jsonl' | cut -d '"' -f 4 > gnd-ak.txt
      - curl --silent --compressed 'https://dataservice.dla-marbach.de/v1/records?q=gnd_id_mv:%2A%20AND%20source:KS&fields=gnd_id_mv&format=jsonl' | cut -d '"' -f 4 > gnd-ks.txt
      - curl --silent --compressed 'https://dataservice.dla-marbach.de/v1/records?q=gnd_id_mv:%2A%20AND%20source:PE&fields=gnd_id_mv&format=jsonl' | cut -d '"' -f 4 > gnd-pe.txt
    preconditions:
      - command -v curl

  lobid-download:
    desc: Bulk-Download der GND als JSON-Lines über lobid-gnd
    dir: data
    cmds:
      - curl -A "DLA Marbach OPAC Enrichment; mailto:dla@felixlohmeier.de" --silent --compressed 'https://lobid.org/gnd/search?q=type:Work&format=jsonl' | gzip > lobid-download-ak.jsonl.gz
      - curl -A "DLA Marbach OPAC Enrichment; mailto:dla@felixlohmeier.de" --silent --compressed 'https://lobid.org/gnd/search?q=type:CorporateBody&format=jsonl' | gzip > lobid-download-ks.jsonl.gz
      - curl -A "DLA Marbach OPAC Enrichment; mailto:dla@felixlohmeier.de" --silent --compressed 'https://lobid.org/gnd/search?q=type:Person&format=jsonl' | gzip > lobid-download-pe.jsonl.gz
    preconditions:
      - command -v curl
      - command -v gzip

  lobid-filter:
    desc: GND-Download reduzieren auf im DLA verwendete GND-IDs
    dir: data
    cmds:
      - python3 ../lobid-filter.py lobid-download-ak.jsonl.gz gnd-ak.txt | gzip > lobid-filtered-ak.jsonl.gz
      - python3 ../lobid-filter.py lobid-download-ks.jsonl.gz gnd-ks.txt | gzip > lobid-filtered-ks.jsonl.gz
      - python3 ../lobid-filter.py lobid-download-pe.jsonl.gz gnd-pe.txt | gzip > lobid-filtered-pe.jsonl.gz
    sources:
      - lobid-download-*.jsonl.gz
      - gnd-*.txt
      - ../lobid-filter.py
    generates:
      - lobid-filtered-*.jsonl.gz
    preconditions:
      - command -v python3
      - command -v gzip

  lobid-transform:
    desc: Gewünschte Daten aus lobid-filtered in einzelne Tabellen schreiben
    dir: data
    cmds:
      - | # Wikidata (sameAs http://www.wikidata.org/entity/)
        gunzip -c lobid-filtered-*.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.abbr == "WIKIDATA") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("http://www.wikidata.org/entity/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-wikidata.tsv
      - | # Deutsche Wikipedia (sameAs https://de.wikipedia.org/wiki/)
        gunzip -c lobid-filtered-*.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.abbr == "dewiki") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://de.wikipedia.org/wiki/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-dewiki.tsv
      - | # Filmportal (sameAs https://www.filmportal.de/)
        gunzip -c lobid-filtered-*.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.id == "https://www.filmportal.de") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://www.filmportal.de/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-filmportal.tsv
      - | # Deutsche Biographie (sameAs https://www.deutsche-biographie.de/)
        gunzip -c lobid-filtered-*.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.id == "https://www.deutsche-biographie.de") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://www.deutsche-biographie.de/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-deutsche-biographie.tsv
      - | # Bild aus Wikimedia Commons (depiction https://commons.wikimedia.org/wiki/Special:FilePath/)
        gunzip -c lobid-filtered-*.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | [$id, .depiction[].id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://commons.wikimedia.org/wiki/Special:FilePath/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-depiction.tsv
    sources:
      - lobid-filtered-*.jsonl.gz
    generates:
      - lobid-*.tsv
    preconditions:
      - command -v jq

  wikidata-extract:
    desc: Mit den aus lobid-gnd ermittelten Wikidata IDs den Wikidata Query Service abfragen
    dir: data
    cmds:
      - cut -f 2 lobid-wikidata.tsv | LC_ALL=POSIX sort -u --version-sort > qid.txt
      - python3 ../wikidata-extract.py qid.txt P18,P109,P856,P2639 | gzip > wikidata-extracted.jsonl.gz
    sources:
      - lobid-wikidata.tsv
      - ../wikidata-extract.py
    generates:
      - qid.txt
      - wikidata-extracted.jsonl.gz
    preconditions:
      - command -v python3
      - command -v gzip

  wikidata-transform:
    desc: Gewünschte Daten aus wikidata-extracted in einzelne Tabellen schreiben
    dir: data
    cmds:
      - | # image (P18 http://commons.wikimedia.org/wiki/Special:FilePath/)
        gunzip -c wikidata-extracted.jsonl.gz \
          | jq -r 'select(.P18 != null) | [.id, .P18] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("http://commons.wikimedia.org/wiki/Special:FilePath/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P18.tsv
      - | # signature (P109 http://commons.wikimedia.org/wiki/Special:FilePath/)
        gunzip -c wikidata-extracted.jsonl.gz \
          | jq -r 'select(.P109 != null) | [.id, .P109] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("http://commons.wikimedia.org/wiki/Special:FilePath/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P109.tsv
      - | # official website (P856)
        gunzip -c wikidata-extracted.jsonl.gz \
          | jq -r 'select(.P856 != null) | [.id, .P856] | @tsv' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P856.tsv
      - | # Filmportal ID (P2639)
        gunzip -c wikidata-extracted.jsonl.gz \
          | jq -r 'select(.P2639 != null) | [.id, .P2639] | @tsv' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P2639.tsv
    sources:
      - wikidata-extracted.jsonl.gz
    generates:
      - wikidata-*.tsv
    preconditions:
      - command -v jq

  commons-extract:
    desc: Batch Abfrage von Wikimedia Commons API query/imageinfo
    dir: data
    cmds:
      - cut -f 2 lobid-depiction.tsv wikidata-P18.tsv wikidata-P109.tsv | sort -u > commons.txt
      - python3 ../commons-extract.py < commons.txt | gzip > commons-extracted.jsonl.gz
    sources:
      - lobid-depiction.tsv
      - wikidata-P18.tsv
      - ../commons-extract.py
    generates:
      - commons-extracted.jsonl.gz
    preconditions:
      - command -v python3
      - command -v gzip

  commons-transform:
    desc: Aus imageinfo eine Rechteangabe generieren
    dir: data
    cmds:
      - |
        gunzip -c commons-extracted.jsonl.gz \
          | jq -r 'try
            .title as $file |
            (.imageinfo // [ {} ])[] |
            (.extmetadata.Artist.value // ""
              | gsub("<.*?>";"")
              | gsub("\\(.*?\\)";"")
              | gsub("\\n";"")
              | gsub("^unknown.*";"";"i")
              | gsub("^anonym.*";"";"i")
              | gsub("^unidentified.*";"";"i")
              | gsub("^unbekannt.*";"";"i")
              | gsub("^various.*";"";"i")
              | gsub("user:";"";"i")
              | gsub("benutzer:";"";"i")
              | gsub("No machine-readable author provided. ";"")
              ) as $artist |
            (.extmetadata.LicenseShortName.value // "Lizenz unbekannt") as $license |
            [
              $file,
              if ($artist | length) > 50 then $artist[:50] + "... | " elif ($artist | length) > 1 then $artist + " | " else "" end
                + "Wikimedia Commons"
                + " | " + $license
            ] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("File:","",$1)}1' \
          | tr -d \" \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > commons-rechte.tsv
    sources:
      - commons-extracted.jsonl.gz
    generates:
      - commons-rechte.tsv
    preconditions:
      - command -v jq

  output:
    desc: Plausibilitätsprüfung und ggf. Kopie der Daten in das Verzeichnis output
    dir: data
    cmds:
      - |
        for f in *.tsv; do
          old="$(wc -l < ../output/${f} || echo 0)"
          new="$(wc -l < ${f})"
          if ((new+10 >= old));
          then
            echo "$f $((new-old))"
          else
            echo 1>&2 "Generierter Cache für ${f} scheint zu klein zu sein! Bitte manuell prüfen."
            echo 1>&2 "Differenz: $((new-old))"
            exit 2
          fi
        done
      - cp *.tsv ../output/
    sources:
      - ./*.tsv
    generates:
      - ../output/*.tsv
