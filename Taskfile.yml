# https://taskfile.dev

version: '3'

set: [errexit, pipefail]

tasks:
  default:
    cmds:
      - task: datendienst
      - task: lobid-download
      - task: lobid-filter
      - task: lobid-transform
      - task: wikidata-extract
      - task: wikidata-transform
      - task: output
    
  datendienst:
    desc: Download von GND-IDs aus dem DLA Datendienst
    dir: data
    cmds:
      - curl --silent --compressed 'https://dataservice.dla-marbach.de/v1/records?q=gnd_id_mv:%2A%20AND%20source:AK&fields=gnd_id_mv&format=jsonl' | cut -d '"' -f 4 > gnd-ak.txt
      - curl --silent --compressed 'https://dataservice.dla-marbach.de/v1/records?q=gnd_id_mv:%2A%20AND%20source:KS&fields=gnd_id_mv&format=jsonl' | cut -d '"' -f 4 > gnd-ks.txt
      - curl --silent --compressed 'https://dataservice.dla-marbach.de/v1/records?q=gnd_id_mv:%2A%20AND%20source:PE&fields=gnd_id_mv&format=jsonl' | cut -d '"' -f 4 > gnd-pe.txt
    preconditions:
      - command -v curl

  lobid-download:
    desc: Bulk-Download der GND als JSON-Lines über lobid-gnd
    dir: data
    cmds:
      - curl -A "DLA Marbach OPAC Enrichment; mailto:dla@felixlohmeier.de" --silent --compressed 'https://lobid.org/gnd/search?q=type:Work&format=jsonl' | gzip > lobid-download-ak.jsonl.gz
      - curl -A "DLA Marbach OPAC Enrichment; mailto:dla@felixlohmeier.de" --silent --compressed 'https://lobid.org/gnd/search?q=type:CorporateBody&format=jsonl' | gzip > lobid-download-ks.jsonl.gz
      - curl -A "DLA Marbach OPAC Enrichment; mailto:dla@felixlohmeier.de" --silent --compressed 'https://lobid.org/gnd/search?q=type:Person&format=jsonl' | gzip > lobid-download-pe.jsonl.gz
    preconditions:
      - command -v curl
      - command -v gzip

  lobid-filter:
    desc: GND-Download reduzieren auf im DLA verwendete GND-IDs
    dir: data
    cmds:
      - python3 ../lobid-filter.py lobid-download-ak.jsonl.gz gnd-ak.txt | gzip > lobid-filtered-ak.jsonl.gz
      - python3 ../lobid-filter.py lobid-download-ks.jsonl.gz gnd-ks.txt | gzip > lobid-filtered-ks.jsonl.gz
      - python3 ../lobid-filter.py lobid-download-pe.jsonl.gz gnd-pe.txt | gzip > lobid-filtered-pe.jsonl.gz
    sources:
      - lobid-download-*.jsonl.gz
      - gnd-*.txt
      - ../lobid-filter.py
    generates:
      - lobid-filtered-*.jsonl.gz
    preconditions:
      - command -v python3
      - command -v gzip

  lobid-transform:
    desc: Gewünschte Daten aus lobid-filtered in einzelne Tabellen extrahieren
    dir: data
    cmds:
      - | # Wikidata (sameAs http://www.wikidata.org/entity/)
        for x in ak ks pe; do
          zcat < lobid-filtered-${x}.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.abbr == "WIKIDATA") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("http://www.wikidata.org/entity/","",$2)}1' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-wikidata-${x}.tsv
        done
      - | # Deutsche Wikipedia (sameAs https://de.wikipedia.org/wiki/)
        for x in ak ks pe; do
          zcat < lobid-filtered-${x}.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.abbr == "dewiki") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://de.wikipedia.org/wiki/","",$2)}1' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-dewiki-${x}.tsv
        done
      - | # Filmportal (sameAs https://www.filmportal.de/)
        for x in pe; do
          zcat < lobid-filtered-${x}.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.id == "https://www.filmportal.de") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://www.filmportal.de/","",$2)}1' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-filmportal-${x}.tsv
        done
      - | # Deutsche Biographie (sameAs https://www.deutsche-biographie.de/)
        for x in pe; do
          zcat < lobid-filtered-${x}.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | .sameAs[] | select (.collection.id == "https://www.deutsche-biographie.de") | [$id, .id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://www.deutsche-biographie.de/","",$2)}1' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-deutsche-biographie-${x}.tsv
        done
      - | # Bild aus Wikimedia Commons (depiction https://commons.wikimedia.org/wiki/Special:FilePath/)
        for x in ak ks pe; do
          zcat < lobid-filtered-${x}.jsonl.gz \
          | jq -r 'try .gndIdentifier as $id | [$id, .depiction[].id] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("https://commons.wikimedia.org/wiki/Special:FilePath/","",$2)}1' \
          | LC_ALL=POSIX sort --stable --key 1,1 \
          > lobid-depiction-${x}.tsv
        done
    sources:
      - lobid-filtered-*.jsonl.gz
    generates:
      - lobid-*.tsv
    preconditions:
      - command -v jq

  wikidata-extract:
    desc: Mit den aus lobid-gnd ermittelten Wikidata IDs den Wikidata Query Service abfragen
    dir: data
    cmds:
      - cut -f 2 lobid-wikidata-ak.tsv | LC_ALL=POSIX sort -u --version-sort > qid-ak.txt
      - cut -f 2 lobid-wikidata-ks.tsv | LC_ALL=POSIX sort -u --version-sort > qid-ks.txt
      - cut -f 2 lobid-wikidata-pe.tsv | LC_ALL=POSIX sort -u --version-sort > qid-pe.txt
      - python3 ../wikidata-extract.py qid-ak.txt P18,P856,P2639 | gzip > wikidata-extracted-ak.jsonl.gz
      - python3 ../wikidata-extract.py qid-ks.txt P18,P856,P2639 | gzip > wikidata-extracted-ks.jsonl.gz
      - python3 ../wikidata-extract.py qid-pe.txt P18,P856,P2639,P109 | gzip > wikidata-extracted-pe.jsonl.gz
    sources:
      - lobid-wikidata-*.tsv
      - ../wikidata-extract.py
    generates:
      - qid-*.txt
      - wikidata-extracted-*.jsonl.gz
    preconditions:
      - command -v python3
      - command -v gzip

  wikidata-transform:
    desc: Gewünschte Daten aus wikidata-extracted in einzelne Tabellen extrahieren
    dir: data
    cmds:
      - | # image (P18 http://commons.wikimedia.org/wiki/Special:FilePath/)
        for x in ak ks pe; do
          zcat < wikidata-extracted-${x}.jsonl.gz \
          | jq -r 'select(.P18 != null) | [.id, .P18] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("http://commons.wikimedia.org/wiki/Special:FilePath/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P18-${x}.tsv
        done
      - | # signature (P109 http://commons.wikimedia.org/wiki/Special:FilePath/)
        for x in pe; do
          zcat < wikidata-extracted-${x}.jsonl.gz \
          | jq -r 'select(.P109 != null) | [.id, .P109] | @tsv' \
          | awk 'BEGIN{FS="\t"; OFS=FS}{gsub("http://commons.wikimedia.org/wiki/Special:FilePath/","",$2)}1' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P109-${x}.tsv
        done
      - | # official website (P856)
        for x in ak ks pe; do
          zcat < wikidata-extracted-${x}.jsonl.gz \
          | jq -r 'select(.P856 != null) | [.id, .P856] | @tsv' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P856-${x}.tsv
        done
      - | # Filmportal ID (P2639)
        for x in ak ks pe; do
          zcat < wikidata-extracted-${x}.jsonl.gz \
          | jq -r 'select(.P2639 != null) | [.id, .P2639] | @tsv' \
          | awk '!x[$0]++' \
          | LC_ALL=POSIX sort --key 1,1 --version-sort \
          > wikidata-P2639-${x}.tsv
        done
    sources:
      - wikidata-extracted-*.jsonl.gz
    generates:
      - wikidata-*.tsv
    preconditions:
      - command -v jq

  output:
    desc: Plausibilitätsprüfung und ggf. Kopie der Daten in das Verzeichnis output
    dir: data
    cmds:
      - |
        for f in *.tsv; do
          old="$(wc -l < ../output/${f} || echo 0)"
          new="$(wc -l < ${f})"
          if ((new+10 >= old));
          then
            echo "$f $((new-old))"
          else
            echo 1>&2 "Generierter Cache für ${f} scheint zu klein zu sein! Bitte manuell prüfen."
            echo 1>&2 "Differenz: $((new-old))"
            exit 2
          fi
        done
      - cp *.tsv ../output/
    sources:
      - ./*.tsv
    generates:
      - ../output/*.tsv
